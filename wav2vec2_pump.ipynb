{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e92699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Instalação \n",
    "# =========================================================\n",
    "# !pip install -U torch torchaudio pytorch-lightning torchmetrics transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a9da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1) Imports, checagem de GPU e configs globais\n",
    "# =========================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import torchmetrics as tm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reprodutibilidade\n",
    "SEED = 42\n",
    "seed_everything(SEED, workers=True)\n",
    "\n",
    "# >>> CAMINHO DO DATASET <<<\n",
    "BASE_PUMP_DIR = r\"D:/dataset pump/-6_dB_pump/pump\"\n",
    "\n",
    "# Hiperparâmetros principais\n",
    "TARGET_SR       = 16000          # Wav2Vec2 base trabalha em 16 kHz\n",
    "MAX_DURATION_S  = 10.0           # audios de 10s \n",
    "MAX_LENGTH      = int(TARGET_SR * MAX_DURATION_S)  # em amostras\n",
    "BATCH_SIZE      = 4              \n",
    "LR              = 1e-4           # LR padrão\n",
    "MAX_EPOCHS      = 30\n",
    "PATIENCE_ES     = 10             # early stopping\n",
    "NUM_WORKERS     = 0              \n",
    "PRECISION       = \"16-mixed\"     # mude para 32 se sua GPU/driver der erro\n",
    "\n",
    "# Checagem rápida de GPU\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f8ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4205 | Normais: 3749 | Anormais: 456\n",
      "Exemplo: 0 D:\\dataset pump\\-6_dB_pump\\pump\\id_00\\normal\\00000000.wav\n",
      "Class weights: [0.56081622838974, 4.610745429992676]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2) Listagem dos arquivos e rótulos (0=normal, 1=abnormal)\n",
    "# =========================================================\n",
    "def list_all_ids(base_pump_dir: str):\n",
    "    X, y = [], []\n",
    "    base = Path(base_pump_dir)\n",
    "    for id_dir in sorted(base.glob(\"id_*\")):\n",
    "        normal_files   = sorted(map(str, (id_dir / \"normal\").glob(\"*.wav\")))\n",
    "        abnormal_files = sorted(map(str, (id_dir / \"abnormal\").glob(\"*.wav\")))\n",
    "        X.extend(normal_files + abnormal_files)\n",
    "        y.extend([0] * len(normal_files) + [1] * len(abnormal_files))\n",
    "    return X, y\n",
    "\n",
    "X_ALL, Y_ALL = list_all_ids(BASE_PUMP_DIR)\n",
    "print(f\"Total: {len(X_ALL)} | Normais: {Y_ALL.count(0)} | Anormais: {Y_ALL.count(1)}\")\n",
    "print(\"Exemplo:\", Y_ALL[0], X_ALL[0])\n",
    "\n",
    "# Pesos das classes (desbalanceamento)\n",
    "class_weights_np = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_ALL),\n",
    "    y=Y_ALL\n",
    ")\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float32)\n",
    "print(\"Class weights:\", class_weights.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87889f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3) Processor do Wav2Vec2\n",
    "# =========================================================\n",
    "# facebook/wav2vec2-base espera áudio mono 16 kHz -> produz tensores prontos para o modelo\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\") # Garante que o input é compatível com o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanhos -> train: 2943, val: 631, test: 631\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 4) Dataset de áudio bruto + DataModule \n",
    "# =========================================================\n",
    "class RawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - Carrega waveform do arquivo\n",
    "    - Converte para mono\n",
    "    - Faz resample para 16 kHz (se preciso)\n",
    "    - Retorna apenas waveform (1D) e label -> padding/truncation ficam no collate_fn\n",
    "    \"\"\"\n",
    "    def __init__(self, files, labels, target_sr=16000):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.target_sr = target_sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path  = self.files[idx]\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        waveform, sr = torchaudio.load(path)  # [C, T]\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # mono [1, T]\n",
    "        if sr != self.target_sr:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.target_sr)(waveform)\n",
    "\n",
    "        return {\"waveform\": waveform.squeeze(0), \"label\": label}\n",
    "\n",
    "\n",
    "class Wav2Vec2DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    # Split: 70/15/15\n",
    "  \n",
    "    def __init__(self, X, y, processor, batch_size=4, num_workers=0, target_sr=16000, max_length=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.target_sr = target_sr\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            self.X, self.y, test_size=0.3, random_state=SEED, stratify=self.y\n",
    "        )\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
    "        )\n",
    "        self.train_ds = RawAudioDataset(X_train, y_train, self.target_sr)\n",
    "        self.val_ds   = RawAudioDataset(X_val,   y_val,   self.target_sr)\n",
    "        self.test_ds  = RawAudioDataset(X_test,  y_test,  self.target_sr)\n",
    "        print(f\"Tamanhos -> train: {len(self.train_ds)}, val: {len(self.val_ds)}, test: {len(self.test_ds)}\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        waveforms = [b[\"waveform\"].squeeze(0).numpy() for b in batch]\n",
    "        labels    = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            waveforms,\n",
    "            sampling_rate=self.target_sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\" if self.max_length is not None else \"longest\",\n",
    "            truncation=True if self.max_length is not None else False,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_values\":  inputs[\"input_values\"],                     # [B, T]\n",
    "            \"attention_mask\":inputs.get(\"attention_mask\", None),         \n",
    "            \"labels\":        labels\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, collate_fn=self.collate_fn, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, collate_fn=self.collate_fn, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, collate_fn=self.collate_fn, pin_memory=True)\n",
    "\n",
    "\n",
    "datamodule = Wav2Vec2DataModule(\n",
    "    X_ALL, Y_ALL,\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_sr=TARGET_SR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42097355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5) LightningModule com Wav2Vec2ForSequenceClassification\n",
    "# =========================================================\n",
    "class LitWav2Vec2Classifier(LightningModule):\n",
    "    def __init__(self, lr=1e-4, class_weights=None, model_name=\"facebook/wav2vec2-base\"):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"class_weights\"])\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "        try:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # class weights \n",
    "        if class_weights is not None:\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "        # Métricas\n",
    "        self.train_acc = tm.classification.MulticlassAccuracy(num_classes=2)\n",
    "        self.val_acc   = tm.classification.MulticlassAccuracy(num_classes=2)\n",
    "        self.val_auc   = tm.classification.MulticlassAUROC(num_classes=2)\n",
    "        self.test_acc  = tm.classification.MulticlassAccuracy(num_classes=2)\n",
    "        self.test_auc  = tm.classification.MulticlassAUROC(num_classes=2)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        input_values = self._ensure_bt(input_values)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = self._ensure_bt(attention_mask)\n",
    "            return self.model(input_values=input_values, attention_mask=attention_mask)\n",
    "        else:\n",
    "            return self.model(input_values=input_values)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        outputs = self(\n",
    "            input_values=batch[\"input_values\"],\n",
    "            attention_mask=batch.get(\"attention_mask\", None)  # pode ser None\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss   = self.loss_fn(logits, batch[\"labels\"])\n",
    "        preds  = torch.argmax(logits, dim=1)\n",
    "\n",
    "        if stage == \"train\":\n",
    "            self.train_acc.update(preds, batch[\"labels\"])\n",
    "            self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        elif stage == \"val\":\n",
    "            self.val_acc.update(preds, batch[\"labels\"])\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            self.val_auc.update(probs, batch[\"labels\"])\n",
    "            self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        else:  # test\n",
    "            self.test_acc.update(preds, batch[\"labels\"])\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            self.test_auc.update(probs, batch[\"labels\"])\n",
    "            self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_acc\", self.train_acc.compute(), prog_bar=True)\n",
    "        self.train_acc.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"val\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"val_acc\", self.val_acc.compute(), prog_bar=True)\n",
    "        self.log(\"val_auc\", self.val_auc.compute(), prog_bar=True)\n",
    "        self.val_acc.reset()\n",
    "        self.val_auc.reset()\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_bt(x):\n",
    "        # Remove dimensões extras tipo [1, B, T] -> [B, T]\n",
    "        if x.dim() > 2 and x.size(0) == 1:\n",
    "            x = x.squeeze(0)\n",
    "        if x.dim() == 3 and x.size(1) == 1:  # [B,1,T] -> [B,T]\n",
    "            x = x.squeeze(1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test_acc\", self.test_acc.compute())\n",
    "        self.log(\"test_auc\", self.test_auc.compute())\n",
    "        self.test_acc.reset()\n",
    "        self.test_auc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a7de68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanhos -> train: 2943, val: 631, test: 631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                              | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | model     | Wav2Vec2ForSequenceClassification | 94.6 M | eval \n",
      "1 | loss_fn   | CrossEntropyLoss                  | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy                | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy                | 0      | train\n",
      "4 | val_auc   | MulticlassAUROC                   | 0      | train\n",
      "5 | test_acc  | MulticlassAccuracy                | 0      | train\n",
      "6 | test_auc  | MulticlassAUROC                   | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "94.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.6 M    Total params\n",
      "378.276   Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "223       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a305f0949bc44b04abd17a026aabf6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39c821dbdee4a7f95964383791904ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2502b855fb024b4ea8c5436e13a7774d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d90704dbccf4bed8703422c64713ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a3eb40c3a44146bfc6cca85e91fa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242876ab599d49f399078c9cdd5d7e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f2ba1b2cbf40bbb2a08715cc4bfdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82d102f876947a280ada07c1601d364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4af0af2391451ab4d925d125e9a31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9593fa6e882e4949a692d0e0bf5d7a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f250af318cd40bfb947b382c9a02da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fb095212504b24b04ac225049b07a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa45a2cef9e94177a294f25aa60d0423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded04b36e99040d2ad82015ca1db0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8815e1d89bed4164a26348d68bc8370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca2a03df07d423992a6e036956ad07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab5b946fffb4799997bf3fd59b1cf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126272828a6b4f4995bbc11bdb582a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26d4a63c5a84cc7bd47a6bd19c4fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd7ebb3fe6240d38ec35db34771fa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7921377c30c453faacc761247649594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b314816b444828951c5b4e7a585956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df7b08a8ee4301804d68ecc29134cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2efb8b3db348cb86f9f706aaebc3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfe75b145124a0b854187b592e9fdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad20181b44c4f80b168e15783dd4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc4371f1a7844849b5cb0df89a3b2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e44c5a5d79641da9e6ea2a7ab5c2416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0d906457249c18b9b3c37bef4f015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7b493bada34ebb9ab399fded8582a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74be6684cb3e4a19b27f918ff4a0fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e85ec21e4849938876ebbdabfd4ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor checkpoint: d:\\dataset pump\\lightning_logs\\version_26\\checkpoints\\wav2vec2-best-epoch=25-val_loss=0.5996.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanhos -> train: 2943, val: 631, test: 631\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5422885ae3a34c179cccea2d61ab8028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.5            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_auc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.5            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5845102071762085     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.5           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_auc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.5           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5845102071762085    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5845102071762085, 'test_acc': 0.5, 'test_auc': 0.5}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 6) Treinador, callbacks e treino\n",
    "# =========================================================\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    filename=\"wav2vec2-best-{epoch:02d}-{val_loss:.4f}\"\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=PATIENCE_ES\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "model = LitWav2Vec2Classifier(\n",
    "    lr=LR,\n",
    "    class_weights=class_weights,\n",
    "    model_name=\"facebook/wav2vec2-base\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    deterministic=True,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, lr_monitor],\n",
    "    accelerator=\"auto\",   # usa GPU se disponível\n",
    "    devices=\"auto\",\n",
    "    precision=PRECISION,  \n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "print(\"Melhor checkpoint:\", checkpoint_cb.best_model_path)\n",
    "\n",
    "# Carrega o melhor modelo e testa\n",
    "best_model = LitWav2Vec2Classifier.load_from_checkpoint(\n",
    "    checkpoint_cb.best_model_path,\n",
    "    lr=LR,\n",
    "    model_name=\"facebook/wav2vec2-base\",\n",
    "    class_weights=class_weights\n",
    ")\n",
    "trainer.test(best_model, datamodule=datamodule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
